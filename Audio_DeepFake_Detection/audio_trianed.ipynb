{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfbab7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a798aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "FIXED_LENGTH = SAMPLE_RATE * 2  # 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3df05447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d288c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class AudioDeepfakeDataset(Dataset):\n",
    "    def __init__(self, files, labels, transform=None):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8ce3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate waveform to a fixed length\n",
    "def pad_waveform(waveform, length=FIXED_LENGTH):\n",
    "    if waveform.shape[1] > length:\n",
    "        return waveform[:, :length]\n",
    "    else:\n",
    "        return F.pad(waveform, (0, length - waveform.shape[1]))\n",
    "\n",
    "transform = lambda x: pad_waveform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a3563c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset paths\n",
    "def load_dataset(data_dir=\"/Users/fenilvadher/Documents/Collage Data/SEM - 6/AI/AI Project/fake_audio\"):\n",
    "    paths, labels = [], []\n",
    "    label_map = {\"real\": 0, \"fake\": 1}\n",
    "    for label_str, label_int in label_map.items():\n",
    "        folder = os.path.join(data_dir, label_str)\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(\".wav\"):\n",
    "                paths.append(os.path.join(folder, file))\n",
    "                labels.append(label_int)\n",
    "    return paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ad2ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, input_length):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        # Compute output shape dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_length)\n",
    "            x = self.pool1(F.relu(self.bn1(self.conv1(dummy_input))))\n",
    "            x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "            self.flattened_size = x.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a07cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_paths, file_labels = load_dataset(\"/Users/fenilvadher/Documents/Collage Data/SEM - 6/AI/AI Project/fake_audio\")\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    file_paths, file_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d5001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional transform: fix length\n",
    "def pad_waveform(waveform, length=16000*2):  # 2 sec fixed length\n",
    "    if waveform.shape[1] > length:\n",
    "        return waveform[:, :length]\n",
    "    else:\n",
    "        return F.pad(waveform, (0, length - waveform.shape[1]))\n",
    "\n",
    "transform = lambda x: pad_waveform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb414bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDeepfakeDataset(train_paths, train_labels, transform=transform)\n",
    "test_dataset = AudioDeepfakeDataset(test_paths, test_labels, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a40aee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = AudioCNN(FIXED_LENGTH).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cae08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 25.9870\n",
      "Epoch 2/50 - Loss: 17.7850\n",
      "Epoch 3/50 - Loss: 10.9809\n",
      "Epoch 4/50 - Loss: 8.9897\n",
      "Epoch 5/50 - Loss: 5.5005\n",
      "Epoch 6/50 - Loss: 4.3007\n",
      "Epoch 7/50 - Loss: 3.7679\n",
      "Epoch 8/50 - Loss: 3.5930\n",
      "Epoch 9/50 - Loss: 3.2931\n",
      "Epoch 10/50 - Loss: 5.9810\n",
      "Epoch 11/50 - Loss: 3.6334\n",
      "Epoch 12/50 - Loss: 3.3944\n",
      "Epoch 13/50 - Loss: 3.8008\n",
      "Epoch 14/50 - Loss: 5.9878\n",
      "Epoch 15/50 - Loss: 3.4583\n",
      "Epoch 16/50 - Loss: 2.5799\n",
      "Epoch 17/50 - Loss: 3.5423\n",
      "Epoch 18/50 - Loss: 2.9140\n",
      "Epoch 19/50 - Loss: 2.4229\n",
      "Epoch 20/50 - Loss: 2.2211\n",
      "Epoch 21/50 - Loss: 2.3235\n",
      "Epoch 22/50 - Loss: 4.3096\n",
      "Epoch 23/50 - Loss: 3.5523\n",
      "Epoch 24/50 - Loss: 3.8224\n",
      "Epoch 25/50 - Loss: 4.6887\n",
      "Epoch 26/50 - Loss: 2.7464\n",
      "Epoch 27/50 - Loss: 3.8396\n",
      "Epoch 28/50 - Loss: 2.6017\n",
      "Epoch 29/50 - Loss: 2.6682\n",
      "Epoch 30/50 - Loss: 2.7672\n",
      "Epoch 31/50 - Loss: 2.0667\n",
      "Epoch 32/50 - Loss: 1.8288\n",
      "Epoch 33/50 - Loss: 1.9619\n",
      "Epoch 34/50 - Loss: 3.4900\n",
      "Epoch 35/50 - Loss: 3.0830\n",
      "Epoch 36/50 - Loss: 1.9323\n",
      "Epoch 37/50 - Loss: 2.2664\n",
      "Epoch 38/50 - Loss: 2.7147\n",
      "Epoch 39/50 - Loss: 2.1647\n",
      "Epoch 40/50 - Loss: 1.7571\n",
      "Epoch 41/50 - Loss: 1.6920\n",
      "Epoch 42/50 - Loss: 1.3380\n",
      "Epoch 43/50 - Loss: 1.8689\n",
      "Epoch 44/50 - Loss: 2.5028\n",
      "Epoch 45/50 - Loss: 2.1143\n",
      "Epoch 46/50 - Loss: 2.2404\n",
      "Epoch 47/50 - Loss: 1.7218\n",
      "Epoch 48/50 - Loss: 2.6794\n",
      "Epoch 49/50 - Loss: 1.9409\n",
      "Epoch 50/50 - Loss: 2.2168\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for waveforms, labels in train_loader:\n",
    "        waveforms = waveforms.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Ensure shape [batch, channels, length]\n",
    "        if waveforms.dim() == 2:\n",
    "            waveforms = waveforms.unsqueeze(1)\n",
    "\n",
    "        outputs = model(waveforms)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf43d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"audio_deepfake_model.pth\")\n",
    "print(\"✅ Model trained and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e2994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
